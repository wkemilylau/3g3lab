{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3G3 Lab: Coding in Visual Cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will explore sparse coding in the primary visual cortex (V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting started\n",
    "\n",
    "You can either run this notebook locally on your own machine or with Google Colab with a Google account. \n",
    "By default we will assume that you are running it locally and you can ignore the instructions in the rest of this cell.\n",
    "\n",
    "Otherwise, **(1) choose **File→Save a copy in Drive and follow the resulting prompts to save a copy in your Google Drive and (2) change `mode` in the next cell from `\"local\"` to `\"colab\"`.**\n",
    "\n",
    "After saving the modified Colab notebook, you should be able to access it inside the folder \"Colab Notebooks\" in Google Drive.\n",
    "The saved Colab notebook will be called \"Copy of 3g3lab.ipynb\" by default. You can change the name at the top of the Colab notebook, as in standard Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"local\" # change to \"colab\" if using Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data structure\n",
    "\n",
    "In general we will deal with a set of images, subimages sampled from those\n",
    "images, a set of bases for the neurons, and the activations of the neurons.\n",
    "For simplicity in this lab all images will be greyscale and square. Our notation and the data\n",
    "formats are as follows. \n",
    "\n",
    "**Notational conventions** We use lowercase, bold\n",
    "letters (e.g. $\\mathbf{v}$) to denote vectors, and capital, bold letters (e.g. $\\mathbf{M}$) to\n",
    "denote matrices. Typewriter font (e.g. `this`) is used to denote\n",
    "Python/NumPy variables or functions. When we refer to the variables in the code, \"vector\" and \"matrix\" mean one- or two-dimensional NumPy arrays. \n",
    "\n",
    "\n",
    "**Images** We could code each image as a separate square matrix with\n",
    "entries representing grey levels, but for efficiency we will code each image as\n",
    "a vector by \"flattening\" the matrix, i.e. by concatenating all the rows. Then\n",
    "*all* the images can be stored in a single matrix $\\mathbf{I}$ where each row\n",
    "represents a different image. The number of images thus equals the number\n",
    "of rows in $\\mathbf{I}$, while the number of $\\mathbf{I}$'s columns equals the number of pixels \n",
    "in the image, $N_{\\sf pix}$.\n",
    "\n",
    "**Subimages (aka image patches)** We will not work on the full images as they can be too\n",
    "large for the simulations. We extract smaller square regions (or \"subimages\")\n",
    "at random locations in the big images. A subimage $\\mathbf{s}$ is again\n",
    "flattened and represented by a (row) vector. The subimage matrix, $\\mathbf{S}$, is in the same format as\n",
    "the matrix of big images, i.e. uses one row per subimage. The number of\n",
    "columns is the number of pixels in each subimage, and the number of rows is the\n",
    "number of subimages. Assuming that all subimages have the same number of pixels, $n_{\\sf pix}$, then the\n",
    "size of $\\mathbf{S}$ is thus $n_{\\sf sub} \\times n_{\\sf pix}$; or in NumPy jargon, its \"shape\" is $(n_{\\sf sub}, n_{\\sf pix})$.\n",
    "\n",
    "**Basis functions** the receptive fields $\\phi_i$ are stored in a\n",
    "matrix $\\mathbf{B}$, in which the $i$-th row is the flattened version of the basis function $\\phi_i$.\n",
    "The number of columns in $\\mathbf{B}$ is the number of pixels in each basis function, which we take to be \n",
    "the same as the number of pixels in each subimage, $n_{\\sf pix}$. The number of\n",
    "rows is the number of basis functions, that is the number of units in the V1 encoding, or more simply yet, the number of V1 model neurons.\n",
    "Thus, the size of $\\mathbf{B}$ is $n_{\\sf bas} \\times n_{\\sf pix}$.\n",
    "\n",
    "**Activations** The neural activations for a subimage are specified by\n",
    "vector $\\mathbf{a}$. The activations over a set of multiple subimages are\n",
    "stored in matrix $\\mathbf{A}$, with column $j$ corresponding to neuron $i$ (and thus associated with basis\n",
    "function $\\phi_i$) and row $i$ corresponding to the $i^{\\sf th}$ subimage.\n",
    "\n",
    "This representation allows simple matrix multiplication to calculate the\n",
    "reconstructed images: $\\widehat{\\mathbf{s}} = \\mathbf{aB}$ and $\\widehat{\\mathbf{S}} =\n",
    "\\mathbf{AB}$.\n",
    "\n",
    "`(n_sub, n_bas)`\n",
    "\n",
    "\n",
    "| notation     | meaning   |     NumPy shape   |\n",
    "|:---------|:-----------:|:----------------:|\n",
    "| $\\mathbf{I}$ | matrix whose rows are the (flattened) full images | `(n_img,n_pix)` |\n",
    "| $\\mathbf{S}$ | matrix whose rows are the (flattened) subimages (or \"image patches\") | `(n_sub,n_pix)` |\n",
    "| $\\mathbf{A}$ | matrix whose rows are the neuronal activations for each subimage |  `(n_bas,n_pix)` |\n",
    "| $\\mathbf{B}$ | matrix whose rows are the (flattened) basis functions |  `(n_bas,n_pix)` |\n",
    "| $n_{\\sf pix}$ | number of pixels in a subimage / basis function | --- |\n",
    "| $n_{\\sf sub}$ | number of subimages | --- |\n",
    "| $n_{\\sf bas}$ | number of basis functions (number of \"V1 neurons\") | --- |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting started with Python/Numpy\n",
    "\n",
    "Most students will be familiar with Python from the [IA computing course](http://teaching.eng.cam.ac.uk/content/engineering-tripos-part-ia-1p4-computing-2020-21).\n",
    "**If you have not used Python/NumPy before, it should be possible\n",
    "to get through the experiment with a help from a demonstrator, but you must work\n",
    "through a tutorial on NumPy (and Python). We suggest the official [Quickstart Tutorial of NumPy](https://numpy.org/doc/stable/user/quickstart.html) or [this one](https://cs231n.github.io/python-numpy-tutorial/) which also captures the basics of Python, Matplotlib (the main Python module for plotting), Jupyter and Google Colab notebooks.**\n",
    "\n",
    "\n",
    "To load the necessary Python modules run the next cell (you can run a cell by `Ctrl+Enter`\n",
    "while you are in it; alternatively `Shift+Enter` runs the cell and proceeds to the next one).\n",
    "Note that `lab` is a custom module written for this notebook. It's documentation can be found [here](https://tachukao.github.io/3g3lab/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "matplotlib.rc('image', cmap='gray')\n",
    "\n",
    "if mode == \"local\":\n",
    "    import lab\n",
    "elif mode == \"colab\":\n",
    "    try:\n",
    "        import lab\n",
    "    except ImportError:\n",
    "        import requests\n",
    "        url = 'https://github.com/tachukao/3g3lab/blob/master/lab.py?raw=true'\n",
    "        r = requests.get(url)\n",
    "        with open('lab.py', 'w') as f:\n",
    "            f.write(r.text)\n",
    "        import lab\n",
    "else:\n",
    "    raise Exception(\"mode must be either local or colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Displaying images (10 min)\n",
    "\n",
    "Three greyscale image sets are provided. To load the images, run the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = lab.load_data(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have loaded the image sets as a Python dictionary, `imgs`, with the following items (key-value pairs):\n",
    "\n",
    "\n",
    "|    key     |  (description of) value |\n",
    "|:----------|:-----------|\n",
    "|`\"I1\"` |  one $512\\times 512$ image stored as a $1 \\times 262144$ matrix |\n",
    "| `\"I2\"` |  nine $512\\times 512$ image stored as a $9 \\times 262144$ matrix |\n",
    "`\"I2w\"` |  modified version of `imgs[\"I2\"]`, discussed later|\n",
    "`\"I3\"` |  nine $512\\times 512$ image stored as a $9 \\times 262144$ matrix|\n",
    "\n",
    "Some are artificially generated images and some are images of real world\n",
    "scenes. \n",
    "\n",
    "Run the next cell to define functions that will plot these images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `lab.plot_all_images` function (defined in the previous cell) to view the images. For example, to display\n",
    "*all* the images in set `imgs[\"I3\"]` you can run\n",
    "\n",
    "`lab.plot_all_images(imgs[\"I3\"])`\n",
    "\n",
    "which will display the images in the `I3` matrix (hereinafter we will use the shorthand `I3` to refer to `imgs[\"I3\"]`, etc, in the text). To display only the first `n` images in `I3`, you can run `lab.plot_all_images(imgs[\"I3\"][0:n])`, instead. \n",
    "\n",
    "Use the next cell to display each of\n",
    "the images data sets `I1`, `I2`, `I2w` and `I3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Which images are natural and which were artificially generated? What do you\n",
    "think the difference is between `I2` and `I2w`? How do you think\n",
    "`I3` was generated?**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compact coding – principal components (30 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will perform principle component analysis (PCA) on sets of images. First run the next cell to define necessary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform PCA on a set of subimages from an\n",
    "image set, using\n",
    "\n",
    "`\n",
    "B, percent = lab.pca3g3(I, sz, n_sub)\n",
    "`\n",
    "\n",
    "where `I` is the image set, each subimage has size `sz` x `sz`,\n",
    "and `n_sub` is the number of subimages to randomly sample from the images.\n",
    "PCA is then performed on these sampled subimages and returns the matrix of\n",
    "basis functions $\\mathbf{B}$ (that is, the principal components) as well as the\n",
    "percent of the total variance in the subimages that each of these components\n",
    "capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Image set `I1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA on the image set `imgs[\"I1\"]`, using `sz=16` and `n_sub=1000`. Then use\n",
    "`plot_all_images` to display the first 16 basis functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, percent = lab.pca3g3(imgs[\"I1\"], 16, 1000)\n",
    "lab.plot_all_images(B[0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the amount of variance that each basis function accounts for on a\n",
    "log-log scale using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.loglog(percent, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**How many principal components would you need to account for 90\\% of the\n",
    "total variance?**</u> (you may find the function `np.cumsum` useful; to find out how it works,\n",
    "run `np.cumsum?`.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some insight into what information the various basis functions encode, one can work out the activation that a neuron responsible for a particular basis function would have, by moving over the text image and multiplying each subimage by a particular basis function.\n",
    "This is equivalent to a two-dimensional filter where the base is the\n",
    "filter.\n",
    "You can use the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by running `lab.overlayimages(image,base)`, which will filter the single image with a single basis function to calculate the\n",
    "activations. These activations are then overlaid onto the original images to\n",
    "highlight the features this neuron is interested in. For example, running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.overlayimages(imgs[\"I1\"][0], B[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will display the image, the activations for the first basis function, and the\n",
    "activation overlaid on the text image as well as the basis function used for\n",
    "filtering. Examine the activations fo rthe first 16 or so basis functions. You\n",
    "may with to write a \"for loop\" to do this automatically. <u>**Can you\n",
    "interpret the sort of image features that some of these bases are selective\n",
    "for?**</u> You may need to zoom in on the images. <u>**Sketch some of the PCs and\n",
    "what your interpretation is.**</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Image set `I3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA on image set `I3` and briefly examine the first 9 basis functions. <u>**Are they interpretable? How many principal components\n",
    "would you need to account for 90% of the variance?**</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Image set `I2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same analysis for the images in set `I2`  for a subimage size of 16 and `nsamp=1000`.\n",
    "Display the first 64 basis functions (in an $8\\times 8$ grid).\n",
    "<u>**Can you interpret them?  How many principal components would you need to account for 90% of the variance?**</u>\n",
    "\n",
    "Again look at the activations of a neuron across some of the images using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.overlayimages(imgs[\"I2\"][0], B[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the first few basis functions, that is for $k=1,2,3,\\ldots$\n",
    "\n",
    "<u>**What sort of decomposition is PCA doing?**</u>\n",
    "We can ask how well the features extracted by PCA match those of visual cortex receptive fields.\n",
    "<u>**What features do the basis functions share with simple cell receptive fields? What features are missing or incorrect?**</u>\n",
    "\n",
    "<u>**Is PCA a convincing model of visual cortex receptive fields?**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sparse distributed coding (45 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this lab will be to determine the optimal bases for sparse\n",
    "coding, examine their properties and the neuronal activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Solving for sparse coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a cost on activations $H(z)=\\log(1+z^2)$, the cost to be minimised for sparse coding\n",
    "from Eq. (4) (of the lab handout) is\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:sparsecost}\n",
    "\\mathcal{C}\\left(\\{\\mathbf{B}_{ij}\\}, \\{\\mathbf{A}_{ki}\\}\\right) = \\frac{1}{K} \\sum_{k=1}^K \\left ( \\sum_j \\left[ \\mathbf{S}_{kj} - \\sum_i \\mathbf{A}_{ki} \\: \\mathbf{B}_{ij} \\right]^2\n",
    "  + \\lambda \\sum_i \\log\\left(1+\\frac{\\mathbf{A}_{ki}^2}{\\sigma^2}\\right) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $K$ is the number of image patches in $\\mathbf{S}$. The first term on the right hand side is the \"reconstruction error\", while the second term is a penalty that encourages the sparsity of the activations $\\mathbf{A}_{ki}$.\n",
    "This cost is to be minimised w.r.t. *both* the basis functions $\\{\\mathbf{B}_{ij} \\}$\n",
    "*and* the activations $\\{\\mathbf{A}_{ki}\\}$. To perform such a minimisation, we can\n",
    "use the following iterative process:\n",
    "\n",
    "\n",
    "- **Step 1:** start with a random set of basis functions $\\{\\mathbf{B}_{ij} \\}$\n",
    "- **Step 2:** given this (non-optimal) fixed set of basis functions, find the\n",
    " set of activations $\\{\\mathbf{A}_{ki}\\}$ that minimises the cost $\\mathcal{C}$, above, now seen as a function of the $\\mathbf{A}_{ki}$ only\n",
    "- **Step 3:** now fix these activations, and adjust the basis\n",
    "functions $\\{\\mathbf{B}_{ij} \\}$ in order to minimise the cost $\\mathcal{C}$, now seen as a function of the $\\mathbf{B}_{ij} $ only\n",
    "\n",
    "This procedure is guaranteed to reduce the overall cost at each step, but in\n",
    "general there is no guarantee that the minimum cost will be reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Step 2: finding the optimal activations given a set of basis functions (30 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function defined above is nonlinear because $H(z)$ is\n",
    "nonlinear. However, it is a smooth function, thus amenable to minimisation\n",
    "using standard gradient descent techniques. Here we will use an iterative\n",
    "technique that makes use of the value of the function as well as its gradient\n",
    "at a given point (for a given set of $a_i$ activations). The core of the method\n",
    "is already implemented for you, but you will need to fill in a\n",
    "function that computes $\\mathcal{C}$ and its gradient (see below).\n",
    "\n",
    "Python will be orders of magnitude more efficient if computations are\n",
    "written in vector form using NumPy arrays. For example, the reconstruction cost (first sum in the right side of the equation defining \n",
    "$\\mathcal{C}$ above) can be written as $\\text{trace}(\\mathbf{E}\\mathbf{E}^\\top) / K $,  where $\\mathbf{E}$ is the error matrix $\\mathbf{E} = \\mathbf{S}-\\mathbf{AB}$ and $\\text{trace}(X) = \\sum_i \\mathbf{X}_{ii}$ (it should be straightforward to verify that $\\text{trace}(\\mathbf{X}\\mathbf{X}^T) = \\sum_{ij} \\mathbf{X}_{ij}^2$. \n",
    "\n",
    "**Before coming to the lab, write down a matrix expression for the gradient of $\\mathcal{C}$, seen as a scalar function of matrix $\\mathbf{A}$.** Note that we have here a scalar function of $KN$ variables (the activations $A_{ki}$), and therefore the gradient is a **matix** of dimension $K \\times N$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\displaystyle\n",
    "\\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}} = \n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{11}}&\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{12}}&  \n",
    " \\ldots &\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{1N}}  \\\\\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{21}}& \n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{22}}&  \n",
    " \\ldots &\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{2N}}  \\\\\n",
    " \\vdots &\n",
    " \\vdots &\n",
    " \\ddots &\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{K1}}& \n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{K2}}&  \n",
    " \\ldots &\n",
    " \\frac{\\partial \\mathcal{C}(\\mathbf{A})}{\\partial \\mathbf{A}_{KN}}  \n",
    " \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "You may want to use the following identities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\text{trace}(\\mathbf{A} \\mathbf{X}^\\top)}{\\partial \\mathbf{X}} = \\frac{\\partial \\text{trace}(\\mathbf{X} \\mathbf{A}^\\top)}{\\partial \\mathbf{X}} = \\mathbf{A}\n",
    "\\qquad {\\sf and} \\qquad\n",
    "\\frac{\\partial \\text{trace}(\\mathbf{X} \\mathbf{A} \\mathbf{X}^\\top)}{\\partial \\mathbf{X}} = \\mathbf{X}\\left(\\mathbf{A} + \\mathbf{A}^\\top\\right).\n",
    "\\end{equation}\n",
    "\n",
    "See also the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).\n",
    "\n",
    "Fill in the functions in the next cell (i.e. write their implementation), so that given a set of activations `A`, a\n",
    "set of basis functions `B`, a subimage matrix `S`, and parameters `sigma` and\n",
    "`lambd` (because `lambda` is a reserved [Python keyword](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) which is actually used below!), they respectively return the cost $\\mathcal{C}$  and the associated\n",
    "gradient $\\partial \\mathcal{C}/\\partial \\mathbf{A}$ (in variables `cost` and `dcost`, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A, B, S, lambd, sigma):\n",
    "    \"\"\"\n",
    "    cost function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: activations with shape (n_sub, n_bas)\n",
    "    B: bases functions (n_bas, n_pix)\n",
    "    S: matrix of image patches (n_sub, n_pix)\n",
    "    lambd: weighs the importance of reconstruction error and sparsity\n",
    "    sigma: activation scale hyper-parameter in the cost function\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    l: average cost per batch (thus l = err + sparsity; see next two lines)\n",
    "    err: reconstruction error per batch\n",
    "    sparsity: sparsity penalty per batch\n",
    "    \"\"\"\n",
    "    # TODO:  Code up the cost function \n",
    "\n",
    "def dcost_A(A, B, S, lambd, sigma):\n",
    "    \"\"\"\n",
    "    gradient of the cost function with respect to A (i.e., dL/dA)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: activations with shape (n_sub, n_bas)\n",
    "    B: bases functions (n_bas, n_pix)\n",
    "    S: image patches (n_sub, n_pix)\n",
    "    lambd: weighs the importance of reconstruction error and sparsity\n",
    "    sigma: activation scale hyper-parameter in the cost function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    g: gradient of the cost function dL/dA\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Code up the gradient dcost/dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have defined `cost` and `dcost_A`, there are two tests that you should\n",
    "perform to make sure your functions are correct. \n",
    "\n",
    "**First**, check that it is\n",
    "self-consistent using `grad_error` (defined in the next cell) which is a function that can check whether\n",
    "your cost and your gradient agree. It does this by comparing the gradient you\n",
    "wrote down to the gradient calculated using a \"finite difference\n",
    "    approximation\", for the cost function that you wrote down. (It applies the test for randomly generated `S`, `A` and `B`, using [`np.random.randn`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html), and for `sigma = 0.3` and `lambd = 0.1`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply (in the following empty cell) `lab.grad_error` to `cost` and `cost_A` to do the consistency check (by inputing `A_or_B` as \"A\").\n",
    "\n",
    "This will print the (appropriately scaled) norm of the difference divided by the norm of the sum\n",
    "as an indication of accuracy.  This number should be below $10^{-5}$ or\n",
    "there is an error in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.grad_error(cost, dcost_A, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Step 3: change the basis functions given the activations (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to update the basis functions to improve the cost, keeping the\n",
    "activations fixed. To do this, we need to differentiate the cost (see Sec. 5.1 above)\n",
    "w.r.t. the basis functions $\\phi_i$ (matrix $\\mathbf{B}$). \n",
    "\n",
    "<u>**Before coming to the lab, work out a matrix expression for the gradient**</u> of $\\mathcal{C}= \\text{trace}(\\mathbf{E} \\mathbf{E}^\\top) / K $ w.r.t. the matrix of basis functions $\\mathbf{B}$, where $\\mathbf{E} = \\mathbf{S} - \\mathbf{AB}$. \n",
    "Note that this gradient is a matrix the same size as $\\mathbf{B}$. \n",
    "\n",
    "You may find the following expressions useful (here $\\mathbf{P}$ and $\\mathbf{Q}$ are matrices):\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\text{trace}(\\mathbf{P}^\\top \\mathbf{X} \\mathbf{Q})}{\\partial \\mathbf{X}} = \\mathbf{PQ}^\\top\n",
    "\\qquad {\\sf and} \\qquad\n",
    "\\frac{\\partial \\text{trace}(\\mathbf{P}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{P})}{\\partial \\mathbf{X}} = 2\\mathbf{X}\\mathbf{PP}^\\top.\n",
    "\\end{equation}\n",
    "\n",
    "Fill in the function `dcost_B` in the next cell (i.e. write its implementation) accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcost_B(A, B, S, lambd, sigma):\n",
    "    \"\"\"\n",
    "    gradient of the cost function with respect to B (i.e., dL/dB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A: activations with shape (n_sub, n_bas)\n",
    "    B: bases functions (n_bas, n_pix)\n",
    "    S: image patches (n_sub, n_pix)\n",
    "    lambd: weighs the importance of reconstruction error and sparsity\n",
    "    sigma: activation scale hyper-parameter in the cost function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    g: gradient of the cost function dL/dB\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO: Code up the graident dcost/dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function, run and make sure the output is below $10^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.grad_error(cost, dcost_B, \"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the gradient w.r.t. $\\mathbf{B}$ in a simple gradient descent procedure,\n",
    "whereby in each step $\\mathbf{B}$ is changed into\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{B} \\leftarrow \\mathbf{B} - \\eta \\frac{\\partial \\mathcal{C}}{\\partial \\mathbf{B}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finding a sparse code (35 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to assemble the full algorithm to determine the sparse code\n",
    "for the image sets `I3` and `I2`. The function `lab.sparseopt` in the next cell contains the majority of the code you need. The code takes as input one of the image sets\n",
    "and runs for a set number of iterations which alternately optimize the\n",
    "activations and basis functions, and finally returns the \"optimal\" basis\n",
    "functions. For example,\n",
    "\n",
    "`\n",
    "B = lab.sparseopt(cost, dcost_A, dcost_B, imgs[\"I3\"], 350)\n",
    "`\n",
    "\n",
    "will perform 350 iterations on set `I3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work through the code in the inner loop of `lab.sparseopt` between the lines clearly marked in the file `lab.py` to understand what it is doing. You can work through the rest of the code at write-up to make sure you understand it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization inner loop first extracts a set of subimages $\\mathbf{S}$ and then\n",
    "\n",
    "1. compute the optimal activations `A` given the current bases `B`\n",
    "and subimages `S` (see Sec. 4.2 above).\n",
    "\n",
    "\n",
    "2. given these activations, optimize the matrix of basis functions `B`\n",
    "by taking one step down the gradient of $\\mathcal{C}$ (see Sec. 4.3 above). \n",
    "Here, you need to iterate over the subimages\n",
    "`S[0]`, `S[1]`, ... and their corresponding activations\n",
    "`A[0]`, `A[1]`, ... to find the gradient `dB` w.r.t. $\\mathbf{B}$\n",
    "for each subimage and average all the gradients out. (Note that `S[0]`, or equivalently `S[0,:]`, is the first row of `S`, etc.)\n",
    "\n",
    "<u>**If you were to run this optimisation recipe now, the values in the basis\n",
    "functions would tend to grow without bound. Why is this?**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep $\\mathbf{B}$ bounded, we normalise them at the end of each iteration:\n",
    "\n",
    "`\n",
    "B = normalise(B,A)\n",
    "`\n",
    "\n",
    "<u>**What do you think this piece of code is meant to achieve?</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the sparse coding model to the `I3` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell run `lab.sparseopt` on the images in `I3` for 350 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = imgs[\"I3\"]\n",
    "B, cost_hist = lab.sparseopt(cost, dcost_A, dcost_B, img, 350, lambd=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the sparse coding model to the `I2` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to run `lab.sparseopt` on the `I2` data set, there would be\n",
    "problems as the image has very different variances along different directions\n",
    "and gradient descent methods are sensitive to this (this is a technical detail\n",
    "that we will not concern ourselves with). Therefore, we provide dataset\n",
    "`I2w` which is a \"spherical\" (aka \"whitened\") version of `I2` in which the variance in\n",
    "different direction has been equalised.\n",
    "\n",
    "For this data set, we will use a larger $\\lambda$ (`lambd=0.2`) and consider larger image patches with dimensions 14 x 14. Now run `lab.sparseopt` on dataset `I2w` for 3000 iterations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = imgs[\"I2w\"]\n",
    "B, ls = lab.sparseopt(cost, dcost_A, dcost_B, img, 3000, lambd=0.2, verbose=True, sz=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Include a few of the resulting basis functions in your report. In what ways are they different from those obtained with PCA? What features do they share with receptive fields of simple cells of the visual cortex?**</u>\n",
    "\n",
    "Again, overlay some of the activations on the images to examine what these\n",
    "units represent. To do this, you will want to filter `I2w` but overlay on `I2`,\n",
    "so use the following (extra term in `lab.overlayimages`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the bases B\n",
    "lab.overlayimages(imgs[\"I2w\"][0], B[1], imgs[\"I2\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Write up the findings of this lab; discuss your interpretation of the results.**</u>\n",
    "\n",
    "<u>**Suggest any further simulations that it would be interesting to perform.**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
